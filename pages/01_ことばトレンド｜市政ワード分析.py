import os
import io
import re 
import json
import urllib.request
from collections import Counter, defaultdict
import pandas as pd

import streamlit as st
import boto3
import zstandard as zstd
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import matplotlib.dates as mdates

import itertools
import networkx as nx
from pyvis.network import Network
import streamlit.components.v1 as components
from networkx.algorithms.community import greedy_modularity_communities

import json as _json
import math


st.set_page_config(page_title="ã“ã¨ã°ãƒˆãƒ¬ãƒ³ãƒ‰ï½œå¸‚æ”¿ãƒ¯ãƒ¼ãƒ‰åˆ†æ", layout="wide", page_icon="âš–ï¸")

# ========================
# è¨­å®š
# ========================
# S3ã®å ´æ‰€
S3_BUCKET =  st.secrets["AWS-KEY"]["DATA_BUCKET_NAME"]
S3_KEY    = "trending-words/mayor-and-council.jsonl.zst"
AWS_REGION = "us-west-2"  
AWS_ACCESS_KEY = st.secrets["AWS-KEY"]["AWS_ACCESS_KEY"]
AWS_SECRET_KEY = st.secrets["AWS-KEY"]["AWS_SECRET_KEY"]

# èªè¨¼æƒ…å ±
# 1) st.secrets["aws"]["AWS_ACCESS_KEY"] / ["AWS_SECRET_KEY"]
# 2) ç’°å¢ƒå¤‰æ•° AWS_ACCESS_KEY_ID / AWS_SECRET_ACCESS_KEY
# 3) IAMãƒ­ãƒ¼ãƒ«ï¼ˆboto3ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰

def make_s3_client():
    return boto3.client(
        "s3",
        aws_access_key_id=AWS_ACCESS_KEY,
        aws_secret_access_key=AWS_SECRET_KEY,
        region_name=AWS_REGION,
    )
# ========================
# ãƒ•ã‚©ãƒ³ãƒˆï¼ˆæ—¥æœ¬èªè¡¨ç¤ºç”¨ï¼‰
# ========================
def get_font_path():
    font_dir = "./font"
    os.makedirs(font_dir, exist_ok=True)
    font_path = os.path.join(font_dir, "NotoSansCJKjp-Regular.otf")
    if not os.path.exists(font_path):
        url = "https://github.com/googlefonts/noto-cjk/raw/main/Sans/OTF/Japanese/NotoSansCJKjp-Regular.otf"
        urllib.request.urlretrieve(url, font_path)
    return font_path

# ========================
# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆS3ã®JSONL.zstï¼‰
# ========================
@st.cache_data(ttl=1800, show_spinner=False)  # 30åˆ†ã‚­ãƒ£ãƒƒã‚·ãƒ¥
def load_preagg_records(bucket: str, key: str):
    """
    S3ä¸Šã® .jsonl.zst ã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒ è§£å‡ã—ã€TextIOWrapperã§UTF-8ã¨ã—ã¦å®‰å…¨ã«è¡Œå˜ä½ã§èª­ã‚€ã€‚
    ãƒ»ãƒãƒ«ãƒãƒã‚¤ãƒˆæ–­ç‰‡ã‚’TextIOWrapperãŒå†…éƒ¨ãƒãƒƒãƒ•ã‚¡ã§å¸åã™ã‚‹ãŸã‚ decode ã‚¨ãƒ©ãƒ¼ã‚’å›é¿
    ãƒ»å·¨å¤§ãƒ•ã‚¡ã‚¤ãƒ«ã§ã‚‚ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚ˆãå‡¦ç†
    """
    s3 = make_s3_client()
    obj = s3.get_object(Bucket=bucket, Key=key)
    body = obj["Body"]  # StreamingBody (file-like)

    dctx = zstd.ZstdDecompressor()
    records = []

    # stream_reader(ãƒã‚¤ãƒˆ) â†’ TextIOWrapper(ãƒ†ã‚­ã‚¹ãƒˆ) ã§è¡Œã”ã¨ã«èª­ã‚€
    with dctx.stream_reader(body) as reader:
        text_stream = io.TextIOWrapper(reader, encoding="utf-8", errors="strict", newline="")
        for line in text_stream:
            line = line.strip()
            if not line:
                continue
            try:
                records.append(json.loads(line))
            except json.JSONDecodeError:
                # ã‚‚ã—æœ«å°¾ã®ä¸å®Œå…¨è¡Œãªã©ãŒã‚ã‚Œã°ã‚¹ã‚­ãƒƒãƒ—ï¼ˆå¿…è¦ãªã‚‰ãƒ­ã‚°ã«å‡ºã™ï¼‰
                # st.warning("ä¸å®Œå…¨ãªJSONè¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ")
                continue

    return records
        
# ========================
# ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰æç”»
# ========================
def draw_wordcloud(freq: dict):
    if not freq:
        st.info("å¯¾è±¡æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹èªå¥ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return
    font_path = get_font_path()
    wc = WordCloud(
        font_path=font_path,
        width=800,
        height=400,
        background_color="white",
        colormap="tab10",
    )
    wc.generate_from_frequencies(freq)
    fig, ax = plt.subplots(figsize=(10, 5))
    ax.imshow(wc, interpolation="bilinear")
    ax.axis("off")
    st.pyplot(fig)

# ========================
# åˆç®—ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# ========================
def aggregate_terms(selected_records):
    counter = Counter()
    total_utter = 0
    for r in selected_records:
        total_utter += int(r.get("utterances", 0))
        for term, cnt in r.get("top_terms", []):
            counter[term] += int(cnt)
    return counter, total_utter

# ========================
# å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹ç¯‰
# ========================
@st.cache_data(ttl=1800, show_spinner=False)
def build_cooccurrence(records, global_freq: Counter, *,
                       top_k_per_doc: int = 30,
                       max_nodes_global: int = 80,
                       weight_mode: str = "binary"  # "binary" or "mincnt"
                       ):
    """
    records: ãƒ•ã‚£ãƒ«ã‚¿å¾Œã®ãƒ¬ã‚³ãƒ¼ãƒ‰ç¾¤
    global_freq: å…¨ä½“ã®é »åº¦ï¼ˆfreq_counterï¼‰
    top_k_per_doc: å„ãƒ¬ã‚³ãƒ¼ãƒ‰å†…ã§ä¸Šä½ä½•èªã‚’å…±èµ·å€™è£œã«ä½¿ã†ã‹
    max_nodes_global: å…¨ä½“é »åº¦ä¸Šä½ã®èªã‚’ã“ã®æ•°ã¾ã§ã«åˆ¶é™ï¼ˆæç”»è»½é‡åŒ–ï¼‰
    weight_mode: "binary"ãªã‚‰åŒæ™‚å‡ºç¾ã§+1, "mincnt"ãªã‚‰min(cnt_i, cnt_j)ã‚’åŠ ç®—
    """
    allowed_terms = set([t for t, _ in global_freq.most_common(max_nodes_global)])

    edge_weight = Counter()
    node_weight = Counter()

    for r in records:
        terms = r.get("top_terms", [])[:top_k_per_doc]
        terms = [(t, int(c)) for t, c in terms if t in allowed_terms]

        for t, c in terms:
            node_weight[t] += c

        for (t1, c1), (t2, c2) in itertools.combinations(terms, 2):
            a, b = sorted((t1, t2))
            if weight_mode == "mincnt":
                edge_weight[(a, b)] += min(c1, c2)
            else:
                edge_weight[(a, b)] += 1

    G = nx.Graph()
    for t, w in node_weight.items():
        G.add_node(t, size=w)

    for (a, b), w in edge_weight.items():
        G.add_edge(a, b, weight=w)

    return G

# ========================
# å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’pyvisã§å¯è¦–åŒ–
# ========================
def render_pyvis_network(
    G: nx.Graph,
    *,
    min_edge_weight: int = 2,
    physics: bool = False,
    height_px: int = 720,
    label_font_size: int = 22,
    enable_clustering: bool = True,
    focus_community: int | None = None,
    drop_isolates: bool = True,             
    keep_largest_component: bool = False    
):
    # ã—ãã„å€¤ã§ã‚¹ãƒªãƒ åŒ–
    H = nx.Graph()
    for n, attrs in G.nodes(data=True):
        H.add_node(n, **attrs)
    for u, v, attrs in G.edges(data=True):
        if int(attrs.get("weight", 1)) >= min_edge_weight:
            H.add_edge(u, v, **attrs)

    # å­¤ç«‹ãƒãƒ¼ãƒ‰ã‚’å‰Šé™¤
    if drop_isolates:
        H.remove_nodes_from(list(nx.isolates(H)))

    # â˜…è¿½åŠ : æœ€å¤§é€£çµæˆåˆ†ã®ã¿æ®‹ã™ï¼ˆä»»æ„ï¼‰
    if keep_largest_component and H.number_of_nodes() > 0:
        comps = list(nx.connected_components(H))
        giant = max(comps, key=len)
        H = H.subgraph(giant).copy()

    # ---- ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£æŠ½å‡º
    comm_map, comms = ({}, [])
    if enable_clustering and H.number_of_nodes() > 0:
        comm_map, comms = detect_communities(H)

    if focus_community is not None and enable_clustering and comms:
        keep = comms[focus_community] if focus_community < len(comms) else set()
        H = H.subgraph(keep).copy()
        if H.number_of_nodes() > 0:
            comm_map, comms = detect_communities(H)

    # â˜… åˆæœŸãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆï¼ˆé‡å¿ƒâ†’ãƒ­ãƒ¼ã‚«ãƒ«ï¼‰ã‚’è¨ˆç®—
    pos = layout_communities_with_warmstart(
        H, comms,
        cluster_k=2.0,     # ã¾ã¨ã¾ã‚Šé–“ã®ãƒãƒé•·ï¼ˆå¤§ãã„ã»ã©é›¢ã‚Œã‚‹ï¼‰
        local_k=0.5,       # ã¾ã¨ã¾ã‚Šå†…ã®å¯†åº¦
        cluster_scale=900, # ã¾ã¨ã¾ã‚Šé–“ã®è·é›¢ã‚¹ã‚±ãƒ¼ãƒ«
        local_scale=550,   # ã¾ã¨ã¾ã‚Šå†…ã®åºƒãŒã‚Šã‚¹ã‚±ãƒ¼ãƒ«
        seed=42
    )

    net = Network(
        height=f"{height_px}px",
        width="100%",
        bgcolor="#ffffff",
        font_color="#1f2937",
        notebook=False,
        directed=False,
    )

    # åˆæœŸåº§æ¨™ã‚’ä¸ãˆã€physicsã¯å‹•ã‹ã™ï¼ˆfixed=False / physics=Trueï¼‰    
    for n, attrs in H.nodes(data=True):
        val = int(attrs.get("size", 1))
        group = comm_map.get(n) if enable_clustering else None
        x, y = pos.get(n, (None, None))
        net.add_node(
            n,
            label=n,
            value=val,
            title=f"{n}: {val}",
            group=group,
            x=x, y=y,
            physics=physics,     # ãƒˆã‚°ãƒ«åæ˜ 
            fixed=not physics    # ç‰©ç†OFFãªã‚‰å›ºå®š
        )

    # ã‚¨ãƒƒã‚¸è¿½åŠ 
    for u, v, attrs in H.edges(data=True):
        w = int(attrs.get("weight", 1))
        net.add_edge(u, v, value=w, title=f"co-occur: {w}")
    # vis.js ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚‚ãƒˆã‚°ãƒ«åæ˜ 
    options = {
        "nodes": {
            "font": {"size": label_font_size, "face": "sans-serif"},
            "scaling": {
                "min": 10, "max": 50,
                "label": {"enabled": True,
                          "min": max(10, label_font_size-6),
                          "max": label_font_size+10}
            }
        },
        "edges": {"smooth": False},
        "interaction": {"tooltipDelay": 100, "hover": True},
        "physics": {
            "enabled": physics,            
            "solver": "forceAtlas2Based",
            "forceAtlas2Based": {
                "gravitationalConstant": -50,
                "springLength": 120,
                "springConstant": 0.08,
                "avoidOverlap": 0.6,
                "damping": 0.45
            },
            "stabilization": {"enabled": physics, "iterations": 300},
            "minVelocity": 0.5,
            "maxVelocity": 30
        }
    }
    
    
    net.set_options(_json.dumps(options))

    html = net.generate_html(notebook=False)
    components.html(html, height=height_px, scrolling=True)


    return comms

def community_meta_graph(H: nx.Graph, comms):
    """
    ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£é–“ã®â€œè¶…ã‚°ãƒ©ãƒ•â€ã‚’ä½œã‚‹ï¼ˆãƒãƒ¼ãƒ‰=ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã€ã‚¨ãƒƒã‚¸=ç›¸äº’æ¥ç¶šã®é‡ã¿åˆè¨ˆï¼‰
    """
    M = nx.Graph()
    for i, nodes in enumerate(comms):
        M.add_node(i, size=len(nodes))
    # ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£é–“ã®ã‚¨ãƒƒã‚¸é‡ã¿ã‚’é›†è¨ˆ
    for u, v, d in H.edges(data=True):
        cu = next(i for i, S in enumerate(comms) if u in S)
        cv = next(i for i, S in enumerate(comms) if v in S)
        if cu == cv:
            continue
        w = int(d.get("weight", 1))
        if M.has_edge(cu, cv):
            M[cu][cv]["weight"] += w
        else:
            M.add_edge(cu, cv, weight=w)
    return M

def layout_communities_with_warmstart(
    H: nx.Graph,
    comms,
    *,
    cluster_k: float = 2.0,     # ã‚¯ãƒ©ã‚¹ã‚¿ï¼ˆé‡å¿ƒï¼‰ã®â€œåºƒãŒã‚Šâ€
    local_k: float = 0.5,       # ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£å†…ã®â€œåºƒãŒã‚Šâ€
    cluster_scale: float = 900, # ã‚¯ãƒ©ã‚¹ã‚¿é–“ã®ã‚¹ã‚±ãƒ¼ãƒ«
    local_scale: float = 550,   # ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£å†…ã®ã‚¹ã‚±ãƒ¼ãƒ«
    seed: int = 42
):
    """
    1) ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£é–“ã®ãƒ¡ã‚¿ã‚°ãƒ©ãƒ•ã‚’ spring_layout
    2) å„ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£å†…ã‚‚ spring_layout
    3) é‡å¿ƒã«ãƒ­ãƒ¼ã‚«ãƒ«é…ç½®ã‚’ã‚ªãƒ•ã‚»ãƒƒãƒˆ â†’ {node: (x, y)} ã‚’è¿”ã™
    """
    if not comms:
        return nx.spring_layout(H, k=local_k, weight="weight", seed=seed)

    # 1) é‡å¿ƒï¼ˆã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ï¼‰é–“ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ
    M = community_meta_graph(H, comms)
    pos_comm = nx.spring_layout(M, k=cluster_k, weight="weight", seed=seed)

    # 2) å„ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£å†…ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ â†’ 3) ã‚ªãƒ•ã‚»ãƒƒãƒˆåˆæˆ
    pos = {}
    for i, nodes in enumerate(comms):
        sub = H.subgraph(nodes)
        local = nx.spring_layout(sub, k=local_k, weight="weight", seed=seed)
        cx, cy = pos_comm.get(i, (0.0, 0.0))
        cx *= cluster_scale
        cy *= cluster_scale
        for n, (x, y) in local.items():
            pos[n] = (cx + x * local_scale, cy + y * local_scale)
    return pos


def detect_communities(G: nx.Graph):
    """é‡ã¿ä»˜ãGreedyã§ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£æŠ½å‡ºã—ã€{node: community_id}, [set(nodes)] ã‚’è¿”ã™"""
    if G.number_of_nodes() == 0:
        return {}, []
    comms = list(greedy_modularity_communities(G, weight="weight"))
    comm_map = {}
    for i, s in enumerate(comms):
        for n in s:
            comm_map[n] = i
    return comm_map, comms

def summarize_communities(G: nx.Graph, comms):
    """å„ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã®ä»£è¡¨èªï¼ˆãƒãƒ¼ãƒ‰sizeé™é †TOP10ï¼‰ã‚’è¿”ã™"""
    summaries = []
    for i, nodes in enumerate(comms):
        rows = sorted(
            [(n, int(G.nodes[n].get("size", 1))) for n in nodes],
            key=lambda x: x[1],
            reverse=True
        )[:10]
        summaries.append({"community": i, "top_terms": rows, "size": len(nodes)})
    return summaries

#ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚°
def layout_by_community(H: nx.Graph, comms, *, intra_k=0.5, spacing=800, seed=42):
    """
    å„ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£å†…ã¯ spring_layout ã§è©°ã‚ã¦é…ç½®ã—ã€
    ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã®â€œå¡Šâ€åŒå£«ã¯å††å‘¨ä¸Šã«ç­‰é–“éš”ã§ã‚ªãƒ•ã‚»ãƒƒãƒˆã€‚
    æˆ»ã‚Šå€¤: {node: (x, y)}
    """
    if not comms:
        return nx.spring_layout(H, k=intra_k, weight="weight", seed=seed)

    pos = {}
    n_comm = len(comms)
    # ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã®ä¸­å¿ƒï¼ˆå††å‘¨ä¸Šï¼‰
    for i, nodes in enumerate(comms):
        angle = 2 * math.pi * i / n_comm
        cx = spacing * math.cos(angle)
        cy = spacing * math.sin(angle)

        # ã‚µãƒ–ã‚°ãƒ©ãƒ•ã‚’å±€æ‰€ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆï¼ˆå¯†ã«ï¼‰
        sub = H.subgraph(nodes)
        local = nx.spring_layout(sub, k=intra_k, weight="weight", seed=seed)

        # ä¸­å¿ƒã¸ã‚ªãƒ•ã‚»ãƒƒãƒˆ
        for n, (x, y) in local.items():
            pos[n] = (x * 300 + cx, y * 300 + cy)  # 300 ã¯å¡Šã®â€œç›´å¾„â€ã‚¹ã‚±ãƒ¼ãƒ«
    return pos

def layout_by_community_grid(H: nx.Graph, comms, *,
                             cluster_spacing=1200,
                             subgraph_scale=500,
                             grid_cols=3,
                             seed=42):
    """
    ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã”ã¨ã« spring_layout ã‚’è¨ˆç®—ã—ã€ã‚°ãƒªãƒƒãƒ‰é…ç½®ã§è·é›¢ã‚’ç©ºã‘ã‚‹
    """
    if not comms:
        return nx.spring_layout(H, k=0.5, weight="weight", seed=seed)

    pos = {}
    for i, nodes in enumerate(comms):
        sub = H.subgraph(nodes)
        # ã‚¯ãƒ©ã‚¹ã‚¿å†…ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ
        local = nx.spring_layout(sub, k=0.5, weight="weight", seed=seed)
        # é‡å¿ƒã‚’ã‚°ãƒªãƒƒãƒ‰ã«é…ç½®
        row, col = divmod(i, grid_cols)
        cx = col * cluster_spacing
        cy = row * cluster_spacing
        for n, (x, y) in local.items():
            pos[n] = (x * subgraph_scale + cx, y * subgraph_scale + cy)
    return pos

#ã‚¨ãƒƒã‚¸æ•°ã®è‡ªå‹•èª¿æ•´
def _edge_count_at_threshold(G: nx.Graph, thr: int) -> int:
    return sum(1 for _, _, d in G.edges(data=True) if int(d.get("weight", 1)) >= thr)

def recommend_min_edge(G: nx.Graph, start_thr: int, *, target_max_edges: int, cap: int,
                       step_back_if_below: bool = True) -> tuple[int, int]:
    """
    ã—ãã„å€¤ã‚’ start_thr ã‹ã‚‰1ãšã¤ä¸Šã’ã€ã‚¨ãƒƒã‚¸æ•°ãŒ target_max_edges ä»¥ä¸‹ã«ãªã£ãŸæ™‚ç‚¹ã§è¿”ã™ã€‚
    ãã®éš›ã€ã‚‚ã— edges < target_max_edges * 0.6 ãªã‚‰ 1æ®µã ã‘æˆ»ã—ã¦ç¢ºå®šã™ã‚‹ã€‚
    ï¼ˆæˆ»ã™ã“ã¨ã§ target_max_edges ã‚’è¶…ãˆã¦ã‚‚æ§‹ã‚ãªã„ã€ã¨ã„ã†ãƒãƒªã‚·ãƒ¼ï¼‰
    æˆ»ã‚Šå€¤: (ç¢ºå®šã—ãã„å€¤, ãã®ã¨ãã®ã‚¨ãƒƒã‚¸æœ¬æ•°)
    """
    thr = start_thr
    edges = _edge_count_at_threshold(G, thr)

    # ä¸Šé™ä»¥ä¸‹ã«åã¾ã‚‹ã¾ã§å¼•ãç· ã‚
    while edges > target_max_edges and thr < cap:
        thr += 1
        edges = _edge_count_at_threshold(G, thr)

    # è½ã¡ã™ãåˆ¤å®šï¼šä¾‹ï¼‰max=200 ã®ã¨ãã€edges < 100 ãªã‚‰ 1 æ®µæˆ»ã™
    if thr > start_thr and edges < target_max_edges * 0.6:
        prev_edges = _edge_count_at_threshold(G, thr - 1)
        return thr - 1, prev_edges

    return thr, edges

# ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã—ãã„å€¤ã‚’ã„ã˜ã£ãŸã‚‰ãƒ•ãƒ©ã‚°ON
def _mark_min_edge_touched():
    st.session_state["min_edge_user_touched"] = True

# å„ç™ºè¨€ã®å¹´æœˆæ—¥å–å¾—
def parse_date_from_chunk_head(chunk_id: str) -> pd.Timestamp | None:
    """
    ä¾‹: '2024å¹´03æœˆ26æ—¥_ä»¤å’Œ6å¹´3æœˆå®šä¾‹è¨˜è€…ä¼šè¦‹_001' â†’ Timestamp('2024-03-26')
    """
    if not chunk_id:
        return None
    head = str(chunk_id).split("_", 1)[0]
    m = re.match(r"^\s*(\d{4})å¹´\s*(\d{1,2})æœˆ\s*(\d{1,2})æ—¥", head)
    if not m:
        return None  # æƒ³å®šå¤–ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãªã‚‰ Noneï¼ˆã‚ã‚Œã°ãƒ­ã‚°ã§ã‚‚OKï¼‰
    y, mo, d = map(int, m.groups())
    try:
        return pd.Timestamp(y, mo, d)
    except Exception:
        return None

    # 2) æ—¥ãªã—ï¼ˆYYYYå¹´MMæœˆ â†’ æœˆåˆæ—¥ã«å¯„ã›ã‚‹ï¼‰
    m = re.match(r"^\s*(\d{4})å¹´\s*(\d{1,2})æœˆ\s*$", head)
    if m:
        y, mo = map(int, m.groups())
        try:
            return pd.Timestamp(y, mo, 1)
        except Exception:
            return None

    return None


# ========================
# UI æœ¬ä½“
# ========================
st.title("âš–ï¸ã“ã¨ã°ãƒˆãƒ¬ãƒ³ãƒ‰ï½œå¸‚æ”¿ãƒ¯ãƒ¼ãƒ‰åˆ†æ")
st.markdown("""
<span style="color:#6b7280">
ç™ºè¨€è€…ã‚„ä¼šè­°ãªã©ã‚’é¸ã‚“ã§ã€ç™ºè¨€ã®å‚¾å‘ã‚’ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§å¯è¦–åŒ–ã—ã¾ã™ã€‚<br>
å¹´åº¦ã‚„ä¼šè­°ã€ç™ºè¨€è€…ã‚’å¤‰ãˆã¦ã€ç™ºè¨€ã®å‚¾å‘ã‚„ç‰¹å¾´ã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚
</span>
""", unsafe_allow_html=True)

st.markdown("""
<style>
/* MultiSelect ã®é¸æŠã‚¿ã‚°ï¼ˆBaseWeb Tagï¼‰ */
.stMultiSelect [data-baseweb="tag"]{
  background-color:#eef3f8 !important;  /* è–„ã„é’ã‚°ãƒ¬ãƒ¼ */
  color:#1f2937 !important;              /* æ¿ƒã„ã‚°ãƒ¬ãƒ¼æ–‡å­— */
  border:1px solid #d1d5db !important;   /* ã‚°ãƒ¬ãƒ¼ç½«ç·š */
}
.stMultiSelect [data-baseweb="tag"] [data-baseweb="tag-close-icon"]{
  color:#6b7280 !important;              /* é–‰ã˜ã‚‹Ã—ã®è‰²ã‚‚æ§ãˆã‚ã« */
}
</style>
""", unsafe_allow_html=True)

with st.spinner("ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™â€¦"):
    try:
        data = load_preagg_records(S3_BUCKET, S3_KEY)
    except Exception as e:
        st.error(f"ã‚µãƒ¼ãƒãƒ¼ã‹ã‚‰ã®èª­è¾¼ã«å¤±æ•—ã—ã¾ã—ãŸã€‚æ™‚é–“ã‚’ãŠã„ã¦å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚")
        st.stop()

if not data:
    st.warning("äº‹å‰é›†è¨ˆãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã—ãŸã€‚")
    st.stop()

# ãƒ•ã‚£ãƒ«ã‚¿ç”¨ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯å€¤åé›†
years          = sorted({rec.get("year") for rec in data if rec.get("year")})
meetings       = sorted({rec.get("meeting_name") for rec in data if rec.get("meeting_name")})
speakers       = sorted({rec.get("speaker") for rec in data if rec.get("speaker")})
speaker_roles  = sorted({rec.get("speaker_role") for rec in data if rec.get("speaker_role")})

# è¡¨ç¤ºé †ã‚’ã€Œè­°å“¡ã€ã€Œå¸‚é•·ã€ã€Œè¡Œæ”¿é–¢ä¿‚è€…ã€ã«å›ºå®šï¼ˆå­˜åœ¨ã—ãªã„ã‚‚ã®ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
desired_order = ["è­°å“¡", "å¸‚é•·", "è¡Œæ”¿é–¢ä¿‚è€…"]
speaker_roles = [r for r in desired_order if r in speaker_roles] + [r for r in speaker_roles if r not in desired_order]

# --- ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç³»ã®åˆæœŸå€¤ ---
# æ¨å¥¨ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼ˆãã‚ï¼‰
DEFAULT_TOPK = 30
DEFAULT_MAXN = 80
DEFAULT_MINEDGE = 2
DEFAULT_LABEL_FONT = 22
DEFAULT_WEIGHT_MODE = "binary"
DEFAULT_PHYSICS = True   

# session_state åˆæœŸåŒ–
ss = st.session_state
ss.setdefault("top_k_per_doc", DEFAULT_TOPK)
ss.setdefault("max_nodes_global", DEFAULT_MAXN)
ss.setdefault("min_edge_weight", DEFAULT_MINEDGE)
ss.setdefault("weight_mode", DEFAULT_WEIGHT_MODE)
ss.setdefault("label_font_size", DEFAULT_LABEL_FONT)
ss.setdefault("physics_on", DEFAULT_PHYSICS)

# è‡ªå‹•èª¿æ•´é–¢é€£ï¼ˆå¿…è¦ãªæ—¢å®šå€¤ï¼‰
ss.setdefault("min_edge_user_touched", False)
ss.setdefault("auto_min_edge", True)
ss.setdefault("target_max_edges", 200)  
ss.setdefault("min_edge_cap", 20)

# --- ãƒ•ã‚£ãƒ«ã‚¿ UIï¼ˆè¤‡æ•°é¸æŠå¯¾å¿œï¼šåˆç®—ã§ãã‚‹ï¼‰---
st.divider()
st.subheader("æ¡ä»¶")

# â€œã™ã¹ã¦â€ ã‚’å®Ÿéš›ã®å…¨é¸æŠã«å±•é–‹ã™ã‚‹å…±é€šé–¢æ•°ï¼ˆMultiSelectç”¨ï¼‰
def expand_all(selected, universe):
    if ("ã™ã¹ã¦" in selected) or (not selected):
        return set(universe)
    return set(selected)

# ---------- 1è¡Œç›®ï¼šç™ºè¨€è€…ç¨®åˆ¥ã¨ ç™ºè¨€è€… ----------

# å½¹è·â†’ç™ºè¨€è€… ã®å¯¾å¿œè¡¨ã®ä½œæˆ
speakers_by_role = defaultdict(set)
for r in data:
    role = r.get("speaker_role")
    sp   = r.get("speaker")
    if role and sp:
        speakers_by_role[role].add(sp)
        
c1, c2 = st.columns(2)

with c1:
    # ç™ºè¨€è€…åŒºåˆ†ï¼šå˜ä¸€é¸æŠï¼ˆselectboxï¼‰
    default_idx = speaker_roles.index("è­°å“¡") if "è­°å“¡" in speaker_roles else 0
    sel_role = st.selectbox("ç™ºè¨€è€…åŒºåˆ†", speaker_roles, index=default_idx, key="role_sb")

# å½¹è·ã«å¿œã˜ãŸç™ºè¨€è€…å€™è£œ
allowed_speakers = sorted(speakers_by_role.get(sel_role, set()))

with c2:
    # ç™ºè¨€è€…ï¼šå˜ä¸€é¸æŠï¼ˆselectboxï¼‰ã€‚"ã™ã¹ã¦" ã‚‚é¸ã¹ã‚‹
    speaker_options = ["ã™ã¹ã¦"] + allowed_speakers
    current = st.session_state.get("speaker_sb", "ã™ã¹ã¦")
    if current not in speaker_options:
        current = "ã™ã¹ã¦"
    sel_speaker = st.selectbox("ç™ºè¨€è€…", speaker_options, index=speaker_options.index(current), key="speaker_sb")

# ---------- 2è¡Œç›®ï¼šç™ºè¨€å¹´ ã¨ ä¼šè­°å ----------
c3, c4 = st.columns(2)
with c3:
    sel_years = st.multiselect("ç™ºè¨€å¹´", ["ã™ã¹ã¦"] + years, default=["ã™ã¹ã¦"])
with c4:
    sel_meet = st.multiselect("ä¼šè­°å", ["ã™ã¹ã¦"] + meetings, default=["ã™ã¹ã¦"])


# --------- ã‚»ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’é›†åˆã«å±•é–‹ ----------
years_set = expand_all(sel_years, years)      # multiselect
meet_set  = expand_all(sel_meet, meetings)    # multiselect

#å˜ä¸€é¸æŠï¼ˆselectboxï¼‰ã¯ã‚¹ã‚«ãƒ©ãƒ¼
if sel_speaker == "ã™ã¹ã¦":
    speaker_set = set(allowed_speakers)
else:
    speaker_set = {sel_speaker}

# --------- ãƒ•ã‚£ãƒ«ã‚¿é©ç”¨ï¼ˆqa_roleæ¡ä»¶ã‚’å‰Šé™¤ï¼speaker_roleã¯å˜ä¸€ä¸€è‡´ï¼‰ ----------
filtered = [
    r for r in data
    if (r.get("year") in years_set)
    and (r.get("meeting_name") in meet_set)
    and (r.get("speaker") in speaker_set)
    and (r.get("speaker_role") == sel_role)
]
# â–¼ ãƒ•ã‚£ãƒ«ã‚¿æ§‹æˆã®ã‚·ã‚°ãƒãƒãƒ£ï¼ˆå¤‰æ›´æ¤œçŸ¥ç”¨ï¼‰
filter_sig = (
    sel_role,
    tuple(sorted(speaker_set)),
    tuple(sorted(years_set)),
    tuple(sorted(meet_set)),
)

# ãƒ•ã‚£ãƒ«ã‚¿ãŒå‰å›ã‹ã‚‰å¤‰ã‚ã£ã¦ã„ã‚Œã°ã€è‡ªå‹•èª¿æ•´ã‚’å†ã³æœ‰åŠ¹åŒ–ã™ã‚‹åˆæœŸçŠ¶æ…‹ã¸æˆ»ã™
if ss.get("last_filter_sig") != filter_sig:
    ss["last_filter_sig"] = filter_sig
    ss["min_edge_user_touched"] = False         # æ‰‹å‹•ãƒ•ãƒ©ã‚°è§£é™¤
    ss["min_edge_weight"] = DEFAULT_MINEDGE     # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«æˆ»ã™ï¼ˆ=è‡ªå‹•èª¿æ•´ãŒèµ°ã‚‹æ¡ä»¶ï¼‰
    
# ===== â–¼ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ ç”»åƒè¡¨ç¤ºéƒ¨ =====

st.divider()
st.subheader("é »å‡ºå˜èª")

freq_counter, total_utterances = aggregate_terms(filtered)
draw_wordcloud(freq_counter)

#st.caption(f"è©²å½“ã‚­ãƒ¼æ•°: {len(filtered)}")
#st.caption(f"åˆç®—å¯¾è±¡ã®ç™ºè¨€ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ï¼ˆutterancesåˆè¨ˆï¼‰: {total_utterances}")

# ===== â–¼ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ è¡¨ç¤ºéƒ¨ =====


# --- ã¾ãšç¾åœ¨å€¤ã§ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ§‹ç¯‰ ---
G = build_cooccurrence(
    filtered,
    freq_counter,
    top_k_per_doc=ss["top_k_per_doc"],
    max_nodes_global=ss["max_nodes_global"],
    weight_mode=ss["weight_mode"],
)

if (
    ss.get("auto_min_edge", True) and
    not ss["min_edge_user_touched"] and
    ss["min_edge_weight"] == DEFAULT_MINEDGE
):
    new_thr, edge_cnt = recommend_min_edge(
        G,
        start_thr=ss["min_edge_weight"],
        target_max_edges=ss.get("target_max_edges", 200),
        cap=ss.get("min_edge_cap", 20),
    )
    if new_thr != ss["min_edge_weight"]:
        ss["min_edge_weight"] = new_thr
    st.caption(f"ç¾åœ¨ã®é–¾å€¤: {ss['min_edge_weight']}ï¼ˆã‚¨ãƒƒã‚¸æ•°: {edge_cnt}ï¼‰")
else:
    st.caption(f"ç¾åœ¨ã®é–¾å€¤: {ss['min_edge_weight']}ï¼ˆã‚¨ãƒƒã‚¸æ•°: {_edge_count_at_threshold(G, ss['min_edge_weight'])}ï¼‰")

st.subheader("å˜èªé–“ã®é–¢ä¿‚æ€§")

# --- ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æç”»ï¼ˆã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£è‰²åˆ†ã‘ï¼‰---
comms = render_pyvis_network(
    G,
    min_edge_weight=ss["min_edge_weight"],
    physics=ss["physics_on"],
    height_px=720,
    label_font_size=ss["label_font_size"],
    enable_clustering=True,
    focus_community=None
)

with st.expander("âš™ï¸ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è¡¨ç¤ºã‚ªãƒ—ã‚·ãƒ§ãƒ³", expanded=False):
    c1, c2, c3, c4, c5 = st.columns(5)
    with c1:
        st.slider("å„ãƒ¬ã‚³ãƒ¼ãƒ‰ã§ä½¿ã†ä¸Šä½èªæ•°", 10, 100, ss["top_k_per_doc"], 5, key="top_k_per_doc")
    with c2:
        st.slider("å…¨ä½“é »åº¦ã®ä¸Šä½Nèªã¾ã§", 30, 200, ss["max_nodes_global"], 10, key="max_nodes_global")
    with c3:
        # â† ã“ã“ã« on_change ã‚’è¿½åŠ 
        st.slider("ã‚¨ãƒƒã‚¸ã®æœ€å°å…±èµ·å›æ•°", 1, 20, ss["min_edge_weight"], 1,
                  key="min_edge_weight", on_change=_mark_min_edge_touched)
    with c4:
        st.selectbox("é‡ã¿ã®å®šç¾©", ["binary", "mincnt"],
                     index=0 if ss["weight_mode"] == "binary" else 1, key="weight_mode")
    with c5:
        st.slider("ãƒ©ãƒ™ãƒ«æ–‡å­—ã‚µã‚¤ã‚º", 12, 36, ss["label_font_size"], 1, key="label_font_size")

    st.toggle("ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã®ç‰©ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æœ‰åŠ¹åŒ–", value=ss["physics_on"], key="physics_on")
    st.caption("â€» ã‚ªãƒ—ã‚·ãƒ§ãƒ³å¤‰æ›´ã§è‡ªå‹•çš„ã«å†æç”»ã•ã‚Œã¾ã™ã€‚")

# ===== â–¼TOP10ï¼ˆå„èªã®æ™‚ç³»åˆ— + ä¸€ç·’ã«å‡ºã¦ããŸãƒ¯ãƒ¼ãƒ‰ï¼‰ =====
st.subheader("é »å‡ºå˜èª TOP10")
with st.expander("å‡ºç¾é »åº¦ã®é«˜ã„å˜èª TOP10", expanded=False):
    TOP_N = 10
    RELATED_K = 5
    threshold = ss["min_edge_weight"]

    top_terms = freq_counter.most_common(TOP_N)
    if not top_terms:
        st.write("ãªã—")
    else:
        for rank, (term, cnt) in enumerate(top_terms, start=1):

            # --- å…±èµ·ï¼ˆé–¢é€£ãƒ¯ãƒ¼ãƒ‰ï¼‰ ---
            cooccur_rows = []
            if G.has_node(term):
                neighbors = []
                for nbr in G.neighbors(term):
                    w = int(G[term][nbr].get("weight", 1))
                    if w >= threshold:
                        neighbors.append((nbr, w))
                neighbors.sort(key=lambda x: x[1], reverse=True)
                cooccur_rows = neighbors[:RELATED_K]

            # --- æ™‚ç³»åˆ—ï¼ˆè­°ä¼šã”ã¨ã®å‡ºç¾æ•°ï¼‰ ---
            ts_rows = []
            for r in filtered:
                local = dict((t, int(c)) for t, c in r.get("top_terms", []))
                c_term = int(local.get(term, 0))
                if c_term <= 0:
                    continue
            
                dt = parse_date_from_chunk_head(r.get("chunk_id", ""))
                if dt is None:
                    continue  # æƒ³å®šå¤–ã‚’é™¤å¤–ï¼ˆå¿…è¦ãªã‚‰ st.warning ãªã©ï¼‰
            
                ts_rows.append({"x": dt, "count": c_term})
            
            if ts_rows:
                df_ts = pd.DataFrame(ts_rows)
                ts_series = (
                    df_ts.groupby("x", dropna=True)["count"].sum().sort_index()
                )
                df_plot = ts_series.reset_index().rename(
                    columns={"x": "æ™‚ç‚¹", "count": "å‡ºç¾æ•°"}
                )
            else:
                df_plot = pd.DataFrame(columns=["æ™‚ç‚¹", "å‡ºç¾æ•°"])
            
            # ---------- è¡¨ç¤º ----------
            with st.expander(f"{rank}ä½ï¼š{term}ï¼ˆé »åº¦ï¼š{cnt}å›ï¼‰", expanded=False):

                # æ™‚ç³»åˆ—ï¼ˆä¸Šï¼‰
                st.markdown("**è©²å½“èªãŒå‡ºç¾ã—ãŸè­°ä¼šã®æ™‚ç³»åˆ—**")
                if not df_plot.empty:
                    # æœˆæ¬¡ã«é›†è¨ˆï¼ˆåŒæœˆå†…ã‚’åˆç®—ï¼‰
                    df_m = df_plot.copy()
                    df_m["YYYY_MM"] = (
                        df_m["æ™‚ç‚¹"]
                        .dt.to_period("M")           # æœˆå˜ä½ã«ä¸¸ã‚ï¼ˆPeriodï¼‰
                        .astype(str)                 # '2023-01' å½¢å¼
                        .str.replace("-", ".", regex=False)  # '2023.01' ã«
                    )
                    df_m = df_m.groupby("YYYY_MM", as_index=False)["å‡ºç¾æ•°"].sum()
                
                    # Streamlitã®ãƒã‚¤ãƒ†ã‚£ãƒ–æŠ˜ã‚Œç·šã§è¦‹ãŸç›®ã‚’ç¶­æŒï¼ˆã‚«ãƒ†ã‚´ãƒªè»¸ï¼‰
                    st.line_chart(df_m.set_index("YYYY_MM")["å‡ºç¾æ•°"])
                else:
                    st.write("æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ãªã—")
                

                # ä¸€ç·’ã«å‡ºã¦ããŸãƒ¯ãƒ¼ãƒ‰ï¼ˆä¸‹ï¼‰
                st.markdown("**ä¸€ç·’ã«ä½¿ã‚ã‚Œã‚„ã™ã„å˜èªï¼ˆä¸Šä½5ä»¶ï¼‰**")
                if cooccur_rows:
                    st.dataframe(
                        {
                            "å˜èª": [n for n, _ in cooccur_rows],
                            "é »åº¦": [w for _, w in cooccur_rows],
                        },
                        use_container_width=True,
                        hide_index=True,
                    )
                else:
                    st.write("ãªã—ï¼ˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å¯¾è±¡å¤– or ã—ãã„å€¤ãŒé«˜ã„ï¼‰")

# ===== â–¼ãƒ•ãƒƒã‚¿ãƒ¼ =====
st.divider()
st.caption("""
âš ï¸ æœ¬ãƒšãƒ¼ã‚¸ã§ã¯è­°ä¼šè­°äº‹éŒ²ãªã©ã‚’å½¢æ…‹ç´ è§£æã—ã€ä¸€èˆ¬çš„ãªèªå¥ã‚’é™¤å¤–ã—ãŸã†ãˆã§é »å‡ºåè©ã‚’æŠ½å‡ºã—ã¦åˆ†æã—ã¦ã„ã¾ã™ã€‚  
âš ï¸ å‡¦ç†è»½æ¸›ã®ãŸã‚ã€å„ç™ºè¨€ã”ã¨ã«ã€Œãã®ç™ºè¨€å†…ã§é »å‡ºã—ãŸä¸Šä½100èªã€ã®ã¿ã‚’é›†è¨ˆå¯¾è±¡ã¨ã—ã¦ã„ã¾ã™ã€‚  
ğŸ™Œ æœ¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯å€‹äººã«ã‚ˆã‚Šé‹å–¶ã•ã‚Œã¦ã„ã¾ã™ã€‚ã”æ”¯æ´ã„ãŸã ã‘ã‚‹æ–¹ã¯ãœã²ã“ã¡ã‚‰ã‹ã‚‰ï¼š  
[ğŸ’› codocã§æ”¯æ´ã™ã‚‹](https://codoc.jp/sites/p8cEFlTZQA/entries/MMZnODc1dw)
""")
